{
 "metadata": {
  "name": "",
  "signature": "sha256:1c2276e9a9b22f498fc0b6d529018e357a0734086d32db2aa4a86d42a7f17ff3"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "[setup tutorial](http://www.abisen.com/spark-from-ipython-notebook.html)\n",
      "\n",
      "I am currently using `spark-1.1.1` and `ipython 2.3.1`\n",
      "\n",
      "You also need to install `py4j`\n",
      "\n",
      "Technically, we need to add spark/python to the search path so that we can load `SparkContext` and `SparkConf`. After that, the `pyspark` will look up `$SPARK_HOME` env variable to find spark so that a SparkContext instance can be created."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from IPython import display"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## configure spark path\n",
      "import os, sys\n",
      "from os import path\n",
      "\n",
      "SPARK_HOME = path.abspath(\"/home/dola/opt/spark-1.1.1/\")\n",
      "os.environ[\"SPARK_HOME\"] = SPARK_HOME\n",
      "sys.path.append(path.join(SPARK_HOME, \"python/\")) ## find pyspark, needs $SPARK_HOME\n",
      "\n",
      "## main entrance to pyspark\n",
      "from pyspark import SparkContext\n",
      "from pyspark import SparkConf\n",
      "import pyspark"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## optionally configure spark settings\n",
      "conf = SparkConf()\n",
      "conf.set(\"spark.executor.memory\", \"32g\", )\n",
      "conf.set(\"spark.cores.max\", \"28\")\n",
      "conf.setAppName(\"spark ipython-notebook\")\n",
      "## initialize sparkcontext\n",
      "sc = SparkContext(\"local\", conf = conf, )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## the control panel\n",
      "display.IFrame(\"http://localhost:4040\", 1000, 300)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "\n",
        "        <iframe\n",
        "            width=\"1000\"\n",
        "            height=300\"\n",
        "            src=\"http://localhost:4040\"\n",
        "            frameborder=\"0\"\n",
        "            allowfullscreen\n",
        "        ></iframe>\n",
        "        "
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 14,
       "text": [
        "<IPython.lib.display.IFrame at 0x7f0bf30087d0>"
       ]
      }
     ],
     "prompt_number": 14
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**I put the code in the modeule startspark.py**\n",
      "\n",
      "**Please note:  In practice, when running on a cluster, you will not want to hardcode master in the program, but rather launch the application with spark-submit and receive it there. However, for local testing and unit tests, you can pass \u201clocal\u201d to run Spark in-process.**"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### An alternative\n",
      "\n",
      "**An alternative is to directly use the spark's frameworks's `bin/pyspark` as it has an option to start ipython notebook, just run:**\n",
      "\n",
      "```IPYTHON=1 ./bin/pyspark # to start ipython``` \n",
      "\n",
      "OR\n",
      "\n",
      "```IPYTHON_OPTS=\"notebook\" ./bin/pyspark # to start ipython notebook```"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}