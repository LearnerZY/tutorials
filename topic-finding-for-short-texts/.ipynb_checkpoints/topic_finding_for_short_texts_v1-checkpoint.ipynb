{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practical Topic Finding for Short-Sentence Texts\n",
    "\n",
    "## 1. Introduction\n",
    "\n",
    "Many analysis applications involve finding topics in corpora of short sentences, such as tweets, short messages, logs and comments. On one hand, the direct insights provided in these topics will serve as the basis of many further analysis, e.g., sentiment scoring or document classificaition. On the other hand, those short texts have some unique characterics that deserve more attentions when applying the traditional topic-finding algorithms to them. Some challenges are\n",
    "- Short texts usually have higher language variability, where the same meaning can be phrased in various ways [[1]](http://www.aclweb.org/anthology/S12-1005). For example, \"dollars\", \"\\$\", \"$$\", \"fee\", \"charges\" may all have similar meanings; but this is harder to capture in shorter sentences due to the limited information of surrounding contexts of each word.\n",
    "- Unlike longer articles such as wiki pages, short comments or tweets may each have a single topic. At first glance it may look like a simplification. But practically it poses challenges when the algorithms to be applied assume a mixture of topics in each document. Forcing a sparse representation usually comes with a price, either computational or performance-related.\n",
    "\n",
    "To make it even more complicated, topic-finding is a multiple-step process, involving preprocessing of texts, vectorization, topic-mining and finally topic representations in keywords. Each step has multiple choices in practice and the different combinations may generate very different results.\n",
    "\n",
    "This article explores the pros and cons of differnet algorithmic decisions in topic-finding, by considering the natures of short texts discussed above. Instead of providing a bird's-eye view of theoritical comparisons, I want to highlight how a practical decision should be made based on the structure of your data and the structure of your topics. A good theoritical review can be found in [[2]](http://anthology.aclweb.org/D/D12/D12-1087.pdf).\n",
    "\n",
    "In the following I use \"toy-like\" artificial data to make those \"blackbox\" models transparent. All the models compared below are implemented in Python [scikit-learn](http://scikit-learn.org/stable/) package. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Topic Finding Models\n",
    "\n",
    "I look into three topic-finding models, namely Latent Dirichlet Allocation ([LDA](https://www.cs.princeton.edu/~blei/papers/BleiNgJordan2003.pdf)), Non-Negative Matrix Factorization ([NMF](http://epubs.siam.org/doi/pdf/10.1137/1.9781611972740.45)), and Truncated Singular Value Decomposition ([SVD](http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.TruncatedSVD.html)). There are many extensions of these traditional models and different implementations. I pick those from [scikit-learn](http://scikit-learn.org/stable/) package. \n",
    "\n",
    "Besides the three traditional topic models, another related approach in finding text structures is document clustering. One of its implementation [KMeans Clustering](http://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html) has also been included in this comparison although it is usually not considered as a topic model approach. You can find interesting discussions on the differences of the models [online](https://www.quora.com/search?q=nmf+vs+lda). The code comments below also provide addtional information on why an implementation decision is made in that way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD, NMF, LatentDirichletAllocation\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before diving into models, let's prepare some sample texts below. Here I generated four artificial corpora for topic-finding. \n",
    "- `clearcut topics`: texts clearly with 2 topics - \"berger-lovers\" and \"sandwich-haters\". It shouldn't be a problem for most methods.\n",
    "- `unbalanced topics`: it has the same 2 topics as above, but the topic distributions are skewed. A real scenario would be finding *outlier* messages or comments from a haystack of normal ones.\n",
    "- `semantic topics`: the corpus has four topics, each for both berger/sandwich lovers and haters. However, in addition to structuring the texts this way, there is another potential dimension that can group \"berger\" vs \"sandwich\" as \"foods topic\" and \"hate\" v.s. \"love\" as \"feelings topic\". Is there any setup that can find topics in this new perspective?\n",
    "- `noisy topics`: as discussed above, short texts may have language variability due to different terms for the same meaning, or even typos. This corpus simulates texts with different typos for two topics. The number of texts in this corpus is smaller than others, so that we can test how the models deal with these ambigurities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def generate_clearcut_topics():\n",
    "    ## for demostration purpose, don't take it personally : )\n",
    "    return np.repeat([\"we love bergers\", \"we hate sandwiches\"], [1000, 1000])\n",
    "\n",
    "def generate_unbalanced_topics():\n",
    "    return np.repeat([\"we love bergers\", \"we hate sandwiches\"], [10, 1000])\n",
    "\n",
    "def generate_semantic_context_topics():\n",
    "    return np.repeat([\"we love bergers\"\n",
    "                      , \"we hate bergers\"\n",
    "                      , \"we love sandwiches\"\n",
    "                      , \"we hate sandwiches\"], 1000)\n",
    "\n",
    "def generate_noisy_topics():\n",
    "    def _random_typos(word, n):\n",
    "        typo_index = np.random.randint(0, len(word), n)\n",
    "        return [word[:i]+\"X\"+word[i+1:] for i in typo_index]\n",
    "    t1 = [\"we love %s\" % w for w in _random_typos(\"bergers\", 15)]\n",
    "    t2 = [\"we hate %s\" % w for w in _random_typos(\"sandwiches\", 15)]\n",
    "    return np.r_[t1, t2]\n",
    "\n",
    "sample_texts = {\n",
    "     \"clearcut topics\": generate_clearcut_topics()\n",
    "    , \"unbalanced topics\": generate_unbalanced_topics()\n",
    "    , \"semantic topics\": generate_semantic_context_topics()\n",
    "    , \"noisy topics\": generate_noisy_topics()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first take a step back and consider what makes a \"good\" topic modelling. Although the standard should depend on the nature of the analysis, there are usually some common understanding. For many cases, the keywords in each topic should be \n",
    "- frequent enough to appear in more than a few documents/sentences. Topics covering only a few examples may not be interesting, unless outlier detection is the purpose. \n",
    "- representative enough to distinguish one topic from another. This sometimes implies *orthogonality* or *independence* among the topics. \n",
    "\n",
    "Some research [[2]](http://anthology.aclweb.org/D/D12/D12-1087.pdf) also propose other criteria such as \n",
    "- co-occurrence of keywords in the same topic should be high, which implies they are coming from the same contexts.\n",
    "- semantic meanings of keywords in a topic should be close, e.g., \"apples\" and \"oranges\" in \"fruits topic\", \"love\" and \"hate\" in \"emotions topic\".\n",
    "\n",
    "Now let's take a look at the implementation of the models to be compared. The four models, namely `NMF`, `SVD`, `LDA` and `KMEANS` are implemented with a single interface `find_topic` below. Each topic model can be combined with two `vectorization` methods, i.e., term-frequence (TF) and term-frequence-inverse-document-frequence ([TFIDF](https://en.wikipedia.org/wiki/Tf%E2%80%93idf)). In general, you should choose TFIDF over TF if you have a lot of common words shared by many texts. Those common words are considered as \"noise\" (or stop-words) that may impair the expressness of real important words in topics. However, this difference between TF and TFIDF is not significant for applications on short sentences, because there is less chance for a word to become \"dominant\" out of many short sentences. Finding other possible vector representations of documents is an active research area. For example, vectorizations based on [word embedding](http://colah.github.io/posts/2014-07-NLP-RNNs-Representations/) models, e.g. [word2vec](https://en.wikipedia.org/wiki/Word2vec) and [doc2vec](http://eng.kifi.com/from-word2vec-to-doc2vec-an-approach-driven-by-chinese-restaurant-process/) have become popular. \n",
    "\n",
    "The following implementation chooses the keywords of topics as the most frequent words in a ***topic-word distribution***, which is usually generated by the topic models or clustering algorithms. However for some models such as SVD or KMEANS clustering, the topic-word matrix could have both positive and negative values, which makes it difficult to be explained as a \"distribution\" and thus choosing the keywords for topics is more ambiguous. For demostration, I choose to pick the keywords as those with significant absolute values and keep the signs with these keywords - the negative words would be prefixed with a ***\"^\"***, such as ***\"^bergers\"***."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def find_topic(texts, topic_model, n_topics, vec_model=\"tf\", thr=1e-2, **kwargs):\n",
    "    \"\"\"Return a list of topics from texts by topic models - for demostration of simple data\n",
    "    texts: array-like strings\n",
    "    topic_model: {\"nmf\", \"svd\", \"lda\", \"kmeans\"} for LSA_NMF, LSA_SVD, LDA, KMEANS (not actually a topic model)\n",
    "    n_topics: # of topics in texts\n",
    "    vec_model: {\"tf\", \"tfidf\"} for term_freq, term_freq_inverse_doc_freq\n",
    "    thr: threshold for finding keywords in a topic model\n",
    "    \"\"\"\n",
    "    ## 1. vectorization\n",
    "    vectorizer = CountVectorizer() if vec_model == \"tf\" else TfidfVectorizer()\n",
    "    text_vec = vectorizer.fit_transform(texts)\n",
    "    words = np.array(vectorizer.get_feature_names())\n",
    "    ## 2. topic finding\n",
    "    topic_models = {\"nmf\": NMF, \"svd\": TruncatedSVD, \"lda\": LatentDirichletAllocation, \"kmeans\": KMeans}\n",
    "    topicfinder = topic_models[topic_model](n_topics, **kwargs).fit(text_vec)\n",
    "    topic_dists = topicfinder.components_ if topic_model is not \"kmeans\" else topicfinder.cluster_centers_\n",
    "    topic_dists /= topic_dists.max(axis = 1).reshape((-1, 1))\n",
    "    ## 3. keywords for topics\n",
    "    ## Unlike other models, LSA_SVD will generate both positive and negative values in topic_word distribution,\n",
    "    ## which makes it more ambiguous to choose keywords for topics. The sign of the weights are kept with the\n",
    "    ## words for a demostration here\n",
    "    def _topic_keywords(topic_dist):\n",
    "        keywords_index = np.abs(topic_dist) >= thr\n",
    "        keywords_prefix = np.where(np.sign(topic_dist) > 0, \"\", \"^\")[keywords_index]\n",
    "        keywords = \" | \".join(map(lambda x: \"\".join(x), zip(keywords_prefix, words[keywords_index])))\n",
    "        return keywords\n",
    "    \n",
    "    topic_keywords = map(_topic_keywords, topic_dists)\n",
    "    return \"\\n\".join(\"Topic %i: %s\" % (i, t) for i, t in enumerate(topic_keywords))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 SVD\n",
    "The [truncated SVD implementation in sklearn](http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.TruncatedSVD.html) is intuitively similiar to PCA algorithm, which tries to find ***orthogonal*** directions that explains the largest ***variances*** in the texts. \n",
    "\n",
    "When applying SVD with TF and TFIDF on the clearcut-topic texts, we got the results below. As discussed, one unique signature of SVD's results is that the words in topics can be both positive and negative. For simple cases, they can be understood as **including** and **excluding** the corresponding word in the topic. \n",
    "\n",
    "For example `\"Topic 1: bergers | ^hate | love | ^sandwiches\"` can be \"intuitively\" explained as the texts that include \"love bergers\" and exclude \"hate sandwiches\". \n",
    "\n",
    "Depending on the random state, your topic results may be different. However, in the topics by SVD, we don't see clear indications of the two topics \"love bergers\" and \"hate sandwiches\". However, it does have topics such as `Topic 3: ^bergers | love`, which means \"love\" but NOT \"bergers\". \n",
    "\n",
    "Interestingly, we may also generate topics such as `Topic 3: bergers | ^hate | ^love | sandwiches`, which captures \"bergers\" and \"sandwiches\" as a \"food\" topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: bergers | hate | love | sandwiches | we\n",
      "Topic 1: bergers | ^hate | love | ^sandwiches\n",
      "Topic 2: bergers | hate | love | sandwiches | ^we\n",
      "Topic 3: ^bergers | love\n"
     ]
    }
   ],
   "source": [
    "print(find_topic(sample_texts[\"clearcut topics\"], \"svd\", 4, vec_model=\"tf\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: bergers | hate | love | sandwiches | we\n",
      "Topic 1: bergers | ^hate | love | ^sandwiches\n",
      "Topic 2: bergers | hate | love | sandwiches | ^we\n",
      "Topic 3: bergers | ^hate | ^love | sandwiches\n"
     ]
    }
   ],
   "source": [
    "print(find_topic(sample_texts[\"clearcut topics\"], \"svd\", 4, vec_model=\"tfidf\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above examples, we set a larger number of topics than expected on purpose, because most of time you don't have the prior knowledge of how many topics there are in your dataset. If we explicitly set the topic number = 2. We get the following result.\n",
    "\n",
    "When explaining the result of SVD, it's important to contrast each topic with previous ones, instead of looking at them separately. So the result below can be explained as *the major difference in the texts are (1) including \"love bergers\" and (2) excluding \"hate sandwiches\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: bergers | hate | love | sandwiches | we\n",
      "Topic 1: bergers | ^hate | love | ^sandwiches\n"
     ]
    }
   ],
   "source": [
    "print(find_topic(sample_texts[\"clearcut topics\"], \"svd\", 2, vec_model=\"tfidf\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try SVD also on the unbalanced topic texts, to see how it performs on detecting minor groups - again both topics have been successfully captured."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: hate | sandwiches | we\n",
      "Topic 1: bergers | ^hate | love | ^sandwiches | we\n",
      "Topic 2: bergers | hate | love | sandwiches | ^we\n"
     ]
    }
   ],
   "source": [
    "print(find_topic(sample_texts[\"unbalanced topics\"], \"svd\", 3, vec_model=\"tf\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***In summary, the topic models generated by SVD***\n",
    "\n",
    "...\n",
    "\n",
    "\n",
    ": from complete vector, flip some bits, then flip more bits, to keep the directions orthogonal\n",
    "- easy to understand, but hard to interept its results. Because there are both positive and negative values, so cannot be interepreted as a probability distribution\n",
    "- just like PCA: finding orthogonal directions that explains most of the varieties in texts\n",
    "- could be useful when doing document clustering/classification because on redundant features?? all orthogonal and have bigger chance to be independent features??\n",
    "- limit: only so many number of topics ...., \n",
    "- co-occurance words not necessarily have similiar weights (both high or both low) in the same topic, they could actually have different signs\n",
    "- it is seldomly useful to use learned topics invidiually, but might be useful as a whole\n",
    "- might be useful to find minor topics in unbalanced texts if the group size is not too small"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 LDA: try to glue similiar words\n",
    "- find topics that is interetable to human beings: similiar words grouped together\n",
    "- cooccured words tend to be grouped together as much as they can be\n",
    "- it could be problem for noisy inputs, or same meaning with variety of words\n",
    "- not so good at finding unbalanced minor topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: bergers | love | we\n",
      "Topic 1: bergers | love | we\n",
      "Topic 2: hate | sandwiches | we\n",
      "Topic 3: bergers | love | we\n"
     ]
    }
   ],
   "source": [
    "print(find_topic(sample_texts[\"clearcut topics\"], \"lda\", 4, vec_model=\"tf\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: bergers | love | we\n",
      "Topic 1: hate | sandwiches | we\n",
      "Topic 2: bergers | love | we\n",
      "Topic 3: bergers | love | we\n"
     ]
    }
   ],
   "source": [
    "print(find_topic(sample_texts[\"clearcut topics\"], \"lda\", 4, vec_model=\"tfidf\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Minor topics - it has been merged with a bigger topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: hate | sandwiches | we\n",
      "Topic 1: bergers | hate | love | sandwiches | we\n",
      "Topic 2: hate | sandwiches | we\n",
      "Topic 3: bergers | hate | love | sandwiches | we\n"
     ]
    }
   ],
   "source": [
    "print(find_topic(sample_texts[\"unbalanced topics\"], \"lda\", 4, vec_model=\"tf\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: bergerx | bergexs | bergxrs | berxers | bexgers | bxrgers | hate | love | sandwichxs | sandwicxes | sandwixhes | sandwxches | sandxiches | sanxwiches | saxdwiches | sxndwiches | we | xandwiches | xergers\n",
      "Topic 1: bergerx | bergexs | bergxrs | berxers | bexgers | bxrgers | hate | love | sandwichxs | sandwicxes | sandwixhes | sandwxches | sandxiches | sanxwiches | saxdwiches | sxndwiches | we | xandwiches | xergers\n"
     ]
    }
   ],
   "source": [
    "print find_topic(sample_texts[\"noisy topics\"],\"lda\",2, vec_model = \"tfidf\",)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 NMF\n",
    "- sth in btween: also interpretable, trying to find \"independent\" topics as much as possible?? \n",
    "- event work with unbalanced topics\n",
    "- however, not as consistent as LDA when number of topics are getting more (potentially more than latent topics). On the other side, LDA will try to glue smaller topics found previously into big ones\n",
    "- more robust to noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: we\n",
      "Topic 1: hate | sandwiches | we\n",
      "Topic 2: hate | sandwiches | we\n",
      "Topic 3: bergers | love | we\n",
      "Topic 4: hate | sandwiches | we\n"
     ]
    }
   ],
   "source": [
    "print(find_topic(sample_texts[\"clearcut topics\"], \"nmf\", 5, vec_model=\"tf\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: hate | sandwiches | we\n",
      "Topic 1: hate | sandwiches | we\n",
      "Topic 2: hate | sandwiches | we\n",
      "Topic 3: hate | sandwiches | we\n",
      "Topic 4: bergers | love | we\n"
     ]
    }
   ],
   "source": [
    "print(find_topic(sample_texts[\"unbalanced topics\"], \"nmf\", 5, vec_model=\"tfidf\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: bergers | love | we\n",
      "Topic 1: hate | sandwiches | we\n",
      "Topic 2: hate | sandwiches | we\n",
      "Topic 3: hate | sandwiches | we\n",
      "Topic 4: bergers | love | we\n",
      "Topic 5: we\n",
      "Topic 6: bergers | love | we\n",
      "Topic 7: we\n",
      "Topic 8: bergers | love | we\n",
      "Topic 9: sandwiches | we\n",
      "Topic 10: hate | sandwiches | we\n",
      "Topic 11: hate | sandwiches | we\n",
      "Topic 12: hate | we\n",
      "Topic 13: bergers | love | we\n",
      "Topic 14: hate | sandwiches | we\n",
      "Topic 15: bergers | love | we\n",
      "Topic 16: bergers | love | we\n",
      "Topic 17: sandwiches | we\n",
      "Topic 18: hate | sandwiches | we\n",
      "Topic 19: bergers | love | we\n",
      "Topic 20: love | we\n",
      "Topic 21: hate | sandwiches | we\n",
      "Topic 22: bergers | love | we\n",
      "Topic 23: bergers | love\n",
      "Topic 24: bergers | love\n"
     ]
    }
   ],
   "source": [
    "print(find_topic(sample_texts[\"clearcut topics\"], \"nmf\", 25, vec_model=\"tfidf\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: bergers | hate | love | sandwiches | we\n",
      "Topic 1: bergers | hate | love | sandwiches | we\n",
      "Topic 2: bergers | love | we\n",
      "Topic 3: bergers | love | we\n",
      "Topic 4: bergers | hate | love | sandwiches | we\n",
      "Topic 5: bergers | love | we\n",
      "Topic 6: bergers | love | we\n",
      "Topic 7: bergers | love | we\n",
      "Topic 8: hate | sandwiches | we\n",
      "Topic 9: bergers | love | we\n",
      "Topic 10: bergers | love | we\n",
      "Topic 11: bergers | love | we\n",
      "Topic 12: bergers | love | we\n",
      "Topic 13: hate | sandwiches | we\n",
      "Topic 14: bergers | love | we\n",
      "Topic 15: bergers | hate | love | sandwiches | we\n",
      "Topic 16: bergers | love | we\n",
      "Topic 17: bergers | love | we\n",
      "Topic 18: hate | sandwiches | we\n",
      "Topic 19: bergers | love | we\n"
     ]
    }
   ],
   "source": [
    "print(find_topic(sample_texts[\"clearcut topics\"], \"lda\", 20, vec_model=\"tf\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: bergerx | bergexs | bergxrs | berxers | bexgers | bxrgers | love | we | xergers\n",
      "Topic 1: hate | sandwichxs | sandwicxes | sandwixhes | sandwxches | sandxiches | sanxwiches | saxdwiches | sxndwiches | we | xandwiches\n"
     ]
    }
   ],
   "source": [
    "print find_topic(sample_texts[\"noisy topics\"],\"nmf\",2, vec_model = \"tfidf\",)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Finding semantic groups\n",
    "None of them can find sematic similiar group of keywords, e.g., \"love, hate\", \"sandwiches, bergers\"\n",
    "\n",
    "The reason is that short sentences don't have enough repetation of contexts to extract those things. And most models focus on co-occurance instead of context similiarity\n",
    "\n",
    "- SVD did quite well as to capture the dimensions in terms of context ?\n",
    "- LDA tends to find big topics with many co-occurend words\n",
    "- NMF is somewhtere in between??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: bergers | hate | love | sandwiches | we\n",
      "Topic 1: bergers | ^hate | love | ^sandwiches\n",
      "Topic 2: ^bergers | ^hate | love | sandwiches\n",
      "Topic 3: bergers | hate | love | sandwiches | ^we\n"
     ]
    }
   ],
   "source": [
    "print(find_topic(sample_texts[\"semantic topics\"], \"svd\", 4, vec_model=\"tfidf\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: sandwiches | we\n",
      "Topic 1: bergers | we\n",
      "Topic 2: love | we\n",
      "Topic 3: hate | we\n",
      "Topic 4: sandwiches | we\n"
     ]
    }
   ],
   "source": [
    "print(find_topic(sample_texts[\"semantic topics\"], \"nmf\", 5, vec_model=\"tfidf\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: bergers | love | we\n",
      "Topic 1: bergers | love | we\n",
      "Topic 2: love | sandwiches | we\n",
      "Topic 3: bergers | hate | we\n",
      "Topic 4: bergers | love | we\n"
     ]
    }
   ],
   "source": [
    "print(find_topic(sample_texts[\"semantic topics\"], \"lda\", 5, vec_model=\"tfidf\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 KMeans\n",
    "- not really a topic model, it just cluster and label documents\n",
    "- no limit of number of topics\n",
    "- good to find unbalanced minor topics\n",
    "- result is robust to number of topics - there is reduanduncy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: hate | sandwiches | we\n",
      "Topic 1: bergers | love | we\n",
      "Topic 2: hate | sandwiches | we\n",
      "Topic 3: hate | sandwiches | we\n",
      "Topic 4: hate | sandwiches | we\n",
      "Topic 5: hate | sandwiches | we\n",
      "Topic 6: hate | sandwiches | we\n",
      "Topic 7: hate | sandwiches | we\n",
      "Topic 8: hate | sandwiches | we\n",
      "Topic 9: hate | sandwiches | we\n"
     ]
    }
   ],
   "source": [
    "print find_topic(sample_texts[\"unbalanced topics\"],\"kmeans\",10, vec_model = \"tf\",)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selecting a good word vector is essential. e.g., it might be dominated by noises in sentences, e.g., stop words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Summary\n",
    "- KMeans is almost always good to start with because \n",
    "    - it's cheap to run (even with large data, e.g., by MiniBatchKMeans) and \n",
    "    - provides useful information about strucutres.\n",
    "    - can be combined with a variety of vector representations, e.g., tf, tfidf, ngrams, doc2vec\n",
    "    - specially useful for short sentences where it's less usuall that each doc has more than one topic\n",
    "    - on the other hand, it is sentitive to doc represnetation because the clusstering is ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "This is not the end... will update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
