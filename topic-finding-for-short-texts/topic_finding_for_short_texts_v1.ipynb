{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practical Topic Finding for Short Texts\n",
    "\n",
    "## 1. Introduction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD, NMF, LatentDirichletAllocation\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Topic Finding Models\n",
    "\n",
    "There are different implementations of NMF (KL, L2...), different parameter to set. Comparison of these can be found. \n",
    "\n",
    "Putting theory aside, let's see how they work within sklearn\n",
    "\n",
    "Kmeans is not a traditional topic finding model, however, can be used to...., use document vector (e.g., tfidf, word2vec if you have large data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's generate some texts for test. Use artificial texts to help us have a microscope view of the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def generate_clearcut_topics():\n",
    "    ## for demostration purpose, don't take it personally : )\n",
    "    return np.repeat([\"we love bergers\", \"we hate sandwiches\"], [1000, 1000])\n",
    "\n",
    "def generate_unbalanced_topics():\n",
    "    return np.repeat([\"we love bergers\", \"we hate sandwiches\"], [10, 1000])\n",
    "\n",
    "\n",
    "def generate_semantic_context_topics():\n",
    "    return np.repeat([\"we love bergers\"\n",
    "                      , \"we hate bergers\"\n",
    "                      , \"we love sandwiches\"\n",
    "                      , \"we hate sandwiches\"], 1000)\n",
    "\n",
    "# def generate_noisy_topics():\n",
    "#     return np.repeat([\"you, me, he, and everybody love bergers\"\n",
    "#                       , \"you, me, he, she, and everybody hate sandwiches\"], 1000)\n",
    "\n",
    "def generate_noisy_topics():\n",
    "    def _random_typos(word, n):\n",
    "        typo_index = np.random.randint(0, len(word), n)\n",
    "        return [word[:i]+\"X\"+word[i+1:] for i in typo_index]\n",
    "    t1 = [\"we love %s\" % w for w in _random_typos(\"bergers\", 15)]\n",
    "    t2 = [\"we hate %s\" % w for w in _random_typos(\"sandwiches\", 15)]\n",
    "    return np.r_[t1, t2]\n",
    "\n",
    "sample_texts = {\n",
    "    \"clearcut topics\": generate_clearcut_topics()\n",
    "    , \"unbalanced topics\": generate_unbalanced_topics()\n",
    "    , \"semantic topics\": generate_semantic_context_topics()\n",
    "    , \"noisy topics\": generate_noisy_topics()\n",
    "}\n",
    "\n",
    "clearcut_topics = generate_clearcut_topics()\n",
    "unbalanced_topics = generate_unbalanced_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we try different texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def find_topic(texts, topic_model, n_topics, vec_model=\"tf\", thr=1e-2, **kwargs):\n",
    "    \"\"\"Return a list of topics from texts by topic models - for demostration of simple data\n",
    "    texts: array-like strings\n",
    "    topic_model: {\"nmf\", \"svd\", \"lda\"} for LSA_NMF, LSA_SVD, LDA\n",
    "    n_topics: # of topics in texts\n",
    "    vec_model: {\"tf\", \"tfidf\"} for term_freq, term_freq_inverse_doc_freq\n",
    "    thr: threshold for finding keywords in a topic model\n",
    "    \"\"\"\n",
    "    ## 1. vectorization\n",
    "    vectorizer = CountVectorizer() if vec_model == \"tf\" else TfidfVectorizer()\n",
    "    text_vec = vectorizer.fit_transform(texts)\n",
    "    words = np.array(vectorizer.get_feature_names())\n",
    "    ## 2. topic finding\n",
    "    topic_models = {\"nmf\": NMF, \"svd\": TruncatedSVD, \"lda\": LatentDirichletAllocation, \"kmeans\": KMeans}\n",
    "    topicfinder = topic_models[topic_model](n_topics, **kwargs).fit(text_vec)\n",
    "    topic_dists = topicfinder.components_ if topic_model is not \"kmeans\" else topicfinder.cluster_centers_\n",
    "    #return topic_dists\n",
    "    topic_dists /= topic_dists.max(axis = 1).reshape((-1, 1))\n",
    "    ## 3. keywords for topics\n",
    "    ## Unlike other models, LSA_SVD will generate both positive and negative values in topic_word distribution,\n",
    "    ## which makes it more ambiguous to choose keywords for topics. The sign of the weights are kept with the\n",
    "    ## words for a demostration here\n",
    "    def _topic_keywords(topic_dist):\n",
    "        keywords_index = np.abs(topic_dist) >= thr\n",
    "        keywords_prefix = np.where(np.sign(topic_dist) > 0, \"\", \"^\")[keywords_index]\n",
    "        keywords = \" | \".join(map(lambda x: \"\".join(x), zip(keywords_prefix, words[keywords_index])))\n",
    "        return keywords\n",
    "    \n",
    "    topic_keywords = map(_topic_keywords, topic_dists)\n",
    "    return \"\\n\".join(\"Topic %i: %s\" % (i, t) for i, t in enumerate(topic_keywords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.16968111,  1.        , -0.42926189],\n",
       "       [ 1.85104976,  1.        ,  1.55310426],\n",
       "       [ 0.49873455, -1.60964749,  1.        ],\n",
       "       [ 0.43113289,  0.388843  ,  1.        ],\n",
       "       [ 0.14845003,  1.        ,  0.67450416]])"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "kmeans = KMeans(5, ).fit(np.random.randn(10, 3))\n",
    "kmeans.cluster_centers_ / kmeans.cluster_centers_.max(axis = 1).reshape((-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cluster_topic(texts, n_topics, vec_model=\"tf\", **kwargs):\n",
    "    \"\"\"Return a list of topics from texts by clustering\n",
    "    texts: array-like strings\n",
    "    n_topics: # of topics in texts\n",
    "    vec_model: {\"tf\", \"tfidf\"} for term_freq, term_freq_inverse_doc_freq\n",
    "    \"\"\"\n",
    "    ## 1. vectorization\n",
    "    vectorizer = CountVectorizer() if vec_model == \"tf\" else TfidfVectorizer()\n",
    "    text_vec = vectorizer.fit_transform(texts)\n",
    "    words = np.array(vectorizer.get_feature_names())\n",
    "    ## 2. document clustering\n",
    "    kmeans = KMeans(n_clusters = n_topics).fit(text_vec)\n",
    "    ## 3. find keywords, either by frequent words or cluster centers, we use the latter here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVD: from complete vector, flip some bits, then flip more bits, to keep the directions orthogonal\n",
    "- easy to understand, but hard to interept its results. Because there are both positive and negative values, so cannot be interepreted as a probability distribution\n",
    "- just like PCA: finding orthogonal directions that explains most of the varieties in texts\n",
    "- could be useful when doing document clustering/classification because on redundant features?? all orthogonal and have bigger chance to be independent features??\n",
    "- limit: only so many number of topics ...., \n",
    "- co-occurance words not necessarily have similiar weights (both high or both low) in the same topic, they could actually have different signs\n",
    "- it is seldomly useful to use learned topics invidiually, but might be useful as a whole\n",
    "- might be useful to find minor topics in unbalanced texts if the group size is not too small"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** explain how it works ***\n",
    "\n",
    "difference between tf and tfidf\n",
    "\n",
    "ORTHOGONAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: bergers | hate | love | sandwiches | we\n",
      "Topic 1: bergers | ^hate | love | ^sandwiches\n",
      "Topic 2: bergers | hate | love | sandwiches | ^we\n",
      "Topic 3: ^hate | sandwiches\n"
     ]
    }
   ],
   "source": [
    "print(find_topic(sample_texts[\"clearcut topics\"], \"svd\", 4, vec_model=\"tf\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: bergers | hate | love | sandwiches | we\n",
      "Topic 1: bergers | ^hate | love | ^sandwiches\n",
      "Topic 2: bergers | hate | love | sandwiches | ^we\n",
      "Topic 3: bergers | ^hate | ^love | sandwiches\n"
     ]
    }
   ],
   "source": [
    "print(find_topic(sample_texts[\"clearcut topics\"], \"svd\", 4, vec_model=\"tfidf\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***unbalanced topics***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: hate | sandwiches | we\n",
      "Topic 1: bergers | ^hate | love | ^sandwiches | we\n",
      "Topic 2: ^bergers | ^hate | ^love | ^sandwiches | we\n"
     ]
    }
   ],
   "source": [
    "print(find_topic(sample_texts[\"unbalanced topics\"], \"svd\", 3, vec_model=\"tf\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA: try to glue similiar words\n",
    "- find topics that is interetable to human beings: similiar words grouped together\n",
    "- cooccured words tend to be grouped together as much as they can be\n",
    "- it could be problem for noisy inputs, or same meaning with variety of words\n",
    "- not so good at finding unbalanced minor topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: bergers | love | we\n",
      "Topic 1: bergers | love | we\n",
      "Topic 2: hate | sandwiches | we\n",
      "Topic 3: bergers | love | we\n"
     ]
    }
   ],
   "source": [
    "print(find_topic(sample_texts[\"clearcut topics\"], \"lda\", 4, vec_model=\"tf\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: bergers | love | we\n",
      "Topic 1: hate | sandwiches | we\n",
      "Topic 2: bergers | love | we\n",
      "Topic 3: bergers | love | we\n"
     ]
    }
   ],
   "source": [
    "print(find_topic(sample_texts[\"clearcut topics\"], \"lda\", 4, vec_model=\"tfidf\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Minor topics - it has been merged with a bigger topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: hate | sandwiches | we\n",
      "Topic 1: bergers | hate | love | sandwiches | we\n",
      "Topic 2: hate | sandwiches | we\n",
      "Topic 3: hate | sandwiches | we\n"
     ]
    }
   ],
   "source": [
    "print(find_topic(sample_texts[\"unbalanced topics\"], \"lda\", 4, vec_model=\"tf\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: bergerx | bergexs | bergxrs | berxers | bexgers | bxrgers | hate | love | sandwichex | sandwichxs | sandwicxes | sandwixhes | sandwxches | sanxwiches | sxndwiches | we | xandwiches | xergers\n",
      "Topic 1: bergerx | bergexs | bergxrs | berxers | bexgers | bxrgers | hate | love | sandwichex | sandwichxs | sandwicxes | sandwixhes | sandwxches | sanxwiches | sxndwiches | we | xandwiches | xergers\n"
     ]
    }
   ],
   "source": [
    "print find_topic(sample_texts[\"noisy topics\"],\"lda\",2, vec_model = \"tfidf\",)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NMF\n",
    "- sth in btween: also interpretable, trying to find \"independent\" topics as much as possible?? \n",
    "- event work with unbalanced topics\n",
    "- however, not as consistent as LDA when number of topics are getting more (potentially more than latent topics). On the other side, LDA will try to glue smaller topics found previously into big ones\n",
    "- more robust to noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: hate | sandwiches | we\n",
      "Topic 1: bergers | love | we\n",
      "Topic 2: bergers | love | we\n",
      "Topic 3: hate | sandwiches | we\n",
      "Topic 4: hate | sandwiches | we\n"
     ]
    }
   ],
   "source": [
    "print(find_topic(sample_texts[\"clearcut topics\"], \"nmf\", 5, vec_model=\"tf\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: bergers | love | we\n",
      "Topic 1: hate | sandwiches | we\n",
      "Topic 2: bergers | love | we\n",
      "Topic 3: hate | sandwiches | we\n",
      "Topic 4: hate | sandwiches | we\n"
     ]
    }
   ],
   "source": [
    "print(find_topic(sample_texts[\"unbalanced topics\"], \"nmf\", 5, vec_model=\"tfidf\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: hate | sandwiches | we\n",
      "Topic 1: bergers | love | we\n",
      "Topic 2: hate | sandwiches | we\n",
      "Topic 3: hate | sandwiches | we\n",
      "Topic 4: bergers | love | we\n",
      "Topic 5: hate | sandwiches | we\n",
      "Topic 6: hate | sandwiches | we\n",
      "Topic 7: we\n",
      "Topic 8: bergers | love | we\n",
      "Topic 9: hate | sandwiches | we\n",
      "Topic 10: bergers | love | we\n",
      "Topic 11: bergers | love | we\n",
      "Topic 12: love\n",
      "Topic 13: bergers | love | we\n",
      "Topic 14: bergers | love | we\n",
      "Topic 15: bergers | love | we\n",
      "Topic 16: bergers | we\n",
      "Topic 17: bergers | love | we\n",
      "Topic 18: bergers | love | we\n",
      "Topic 19: bergers | love | we\n",
      "Topic 20: love | we\n",
      "Topic 21: hate | sandwiches | we\n",
      "Topic 22: bergers | love | we\n",
      "Topic 23: hate | sandwiches\n",
      "Topic 24: bergers | love | we\n"
     ]
    }
   ],
   "source": [
    "print(find_topic(sample_texts[\"clearcut topics\"], \"nmf\", 25, vec_model=\"tfidf\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: bergers | hate | love | sandwiches | we\n",
      "Topic 1: bergers | love | we\n",
      "Topic 2: hate | sandwiches | we\n",
      "Topic 3: bergers | love | we\n",
      "Topic 4: bergers | hate | love | sandwiches | we\n",
      "Topic 5: bergers | love | we\n",
      "Topic 6: bergers | love | we\n",
      "Topic 7: bergers | hate | love | sandwiches | we\n",
      "Topic 8: bergers | hate | love | sandwiches | we\n",
      "Topic 9: bergers | love | we\n",
      "Topic 10: bergers | hate | love | sandwiches | we\n",
      "Topic 11: bergers | love | we\n",
      "Topic 12: bergers | love | we\n",
      "Topic 13: bergers | love | we\n",
      "Topic 14: bergers | love | we\n",
      "Topic 15: bergers | love | we\n",
      "Topic 16: bergers | love | we\n",
      "Topic 17: bergers | love | we\n",
      "Topic 18: bergers | love | we\n",
      "Topic 19: bergers | love | we\n"
     ]
    }
   ],
   "source": [
    "print(find_topic(sample_texts[\"clearcut topics\"], \"lda\", 20, vec_model=\"tf\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: hate | sandwichex | sandwichxs | sandwicxes | sandwixhes | sandwxches | sanxwiches | sxndwiches | we | xandwiches\n",
      "Topic 1: bergerx | bergexs | bergxrs | berxers | bexgers | bxrgers | love | we | xergers\n"
     ]
    }
   ],
   "source": [
    "print find_topic(sample_texts[\"noisy topics\"],\"nmf\",2, vec_model = \"tfidf\",)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding semantic groups\n",
    "None of them can find sematic similiar group of keywords, e.g., \"love, hate\", \"sandwiches, bergers\"\n",
    "\n",
    "The reason is that short sentences don't have enough repetation of contexts to extract those things. And most models focus on co-occurance instead of context similiarity\n",
    "\n",
    "- SVD did quite well as to capture the dimensions in terms of context ?\n",
    "- LDA tends to find big topics with many co-occurend words\n",
    "- NMF is somewhtere in between??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: bergers | hate | we\n",
      "Topic 1: bergers | we\n",
      "Topic 2: love | we\n",
      "Topic 3: sandwiches | we\n",
      "Topic 4: hate | we\n"
     ]
    }
   ],
   "source": [
    "print(find_topic(sample_texts[\"semantic topics\"], \"nmf\", 5, vec_model=\"tfidf\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: love | sandwiches | we\n",
      "Topic 1: bergers | love | we\n",
      "Topic 2: bergers | love | we\n",
      "Topic 3: bergers | love | we\n",
      "Topic 4: bergers | hate | we\n"
     ]
    }
   ],
   "source": [
    "print(find_topic(sample_texts[\"semantic topics\"], \"lda\", 5, vec_model=\"tfidf\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: bergers | hate | love | sandwiches | we\n",
      "Topic 1: ^hate | love\n",
      "Topic 2: bergers | ^sandwiches\n",
      "Topic 3: bergers | hate | love | sandwiches | ^we\n"
     ]
    }
   ],
   "source": [
    "print(find_topic(sample_texts[\"semantic topics\"], \"svd\", 4, vec_model=\"tfidf\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KMeans\n",
    "- not really a topic model, it just cluster and label documents\n",
    "- no limit of number of topics\n",
    "- good to find unbalanced minor topics\n",
    "- result is robust to number of topics - there is reduanduncy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: hate | sandwiches | we\n",
      "Topic 1: bergers | love | we\n",
      "Topic 2: hate | sandwiches | we\n",
      "Topic 3: hate | sandwiches | we\n",
      "Topic 4: hate | sandwiches | we\n",
      "Topic 5: hate | sandwiches | we\n",
      "Topic 6: hate | sandwiches | we\n",
      "Topic 7: hate | sandwiches | we\n",
      "Topic 8: hate | sandwiches | we\n",
      "Topic 9: hate | sandwiches | we\n"
     ]
    }
   ],
   "source": [
    "print find_topic(sample_texts[\"unbalanced topics\"],\"kmeans\",10, vec_model = \"tf\",)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selecting a good word vector is essential. e.g., it might be dominated by noises in sentences, e.g., stop words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "- KMeans is almost always good to start with because \n",
    "    - it's cheap to run (even with large data, e.g., by MiniBatchKMeans) and \n",
    "    - provides useful information about strucutres.\n",
    "    - can be combined with a variety of vector representations, e.g., tf, tfidf, ngrams, doc2vec\n",
    "    - specially useful for short sentences where it's less usuall that each doc has more than one topic\n",
    "    - on the other hand, it is sentitive to doc represnetation because the clusstering is ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
